# -*- coding: utf-8 -*-
"""Capston_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uJYvkOV1dsb1nRxCKy8Z4j2CgR4cU91W

**Step 1: Importing all the necessary libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

"""**Step 2: Importing the dataset**"""

# Load the dataset
customer = pd.read_csv('Customer Data.csv')
customer

#too display top 5 data
customer.head()

#too display bottom 5 data
customer.tail()

#too display random 5 data
customer.sample(5)

#shape of dataset
customer.shape

"""**Step 3: Data Preprocessing**"""

#statistics summary for numerical columns
customer.describe()

#basic information about dataset
customer.info()

#too check null values
customer.isnull().sum()

"""***Predictive imputation for MINIMUM_PAYMENTS***"""

customer["MINIMUM_PAYMENTS"]

# Separate rows with and without missing values
missing_data = customer[customer['MINIMUM_PAYMENTS'].isnull()]
non_missing_data = customer[customer['MINIMUM_PAYMENTS'].notnull()]

# Features to use for prediction
features = ['BALANCE', 'PURCHASES', 'PAYMENTS', 'CREDIT_LIMIT', 'TENURE']

# Train-test split
X = non_missing_data[features] # Predictor variables
y = non_missing_data['MINIMUM_PAYMENTS'] # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Split into train and test sets

# Train RandomForestRegressor
model = RandomForestRegressor(random_state=42) # Calling the model
model.fit(X_train, y_train) #Fitting the model

# Predict missing values
predicted_values = model.predict(missing_data[features])

# Fill missing values in the dataset with the predicted values
customer.loc[customer['MINIMUM_PAYMENTS'].isnull(), 'MINIMUM_PAYMENTS'] = predicted_values

# Scale numerical features
standardscaler = StandardScaler() # Calling StandardScaler
scaled_data = scaler.fit_transform(customer.select_dtypes(include=np.number)) # Scale numerical features

customer["MINIMUM_PAYMENTS"]

customer.isnull().sum()

#drop all null values
customer.dropna(inplace = True)
customer.shape

customer.isnull().sum()

"""**Step 4: Exploratory Data Analysis**"""

# Distribution of numerical features
numerical_features = customer.select_dtypes(include=np.number).columns  # Get numerical columns
for feature in numerical_features:  # Loop through each numerical feature
    plt.figure(figsize=(8, 4))  # Set the figure size
    sns.histplot(customer[feature], kde=True, bins=30, color='blue')  # Plot histogram with KDE overlay
    plt.title(f'Distribution of {feature}')  # Title of the plot
    plt.xlabel(feature)  # X-axis label as the feature name
    plt.ylabel('Frequency')  # Y-axis label as 'Frequency'
    plt.show()  # Display the plot

# Drop non-numeric columns
numeric_df = customer.select_dtypes(include=np.number) # Select only numeric columns from the DataFrame

# Plot a correlation heatmap for the numeric features
plt.figure(figsize=(12, 8))  # Set the figure size for better visibility
correlation_matrix = numeric_df.corr()  # Compute the correlation matrix of numeric features
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')  # Plot the heatmap with annotations
plt.title('Correlation Heatmap')  # Add a title to the heatmap
plt.show()  # Display the plot

# Boxplots to check for outliers
for feature in numerical_features:  # Loop through each numerical feature
    plt.figure(figsize=(8, 4))  # Set figure size
    sns.boxplot(x=customer[feature], color='green')  # Create a boxplot to check for outliers
    plt.title(f'Boxplot of {feature}')  # Title of the boxplot
    plt.xlabel(feature)  # X-axis label
    plt.show()  # Display the boxplot

"""**Step 5: Dimensionality Reduction**

*PCA(Principal Component Analysis)*
"""

scaled_data_df = pd.DataFrame(scaled_data)  # Convert to DataFrame
print(scaled_data_df.isnull().sum())  # Check for missing values column-wise

# PCA for reducing dimensions to 2 for visualization
scaled_data_dropped = scaled_data[:, ~np.isnan(scaled_data).any(axis=0)] # Remove columns with NaN values
pca = PCA(n_components=9)  # Specify 9 components for visualization
pca_data = pca.fit_transform(scaled_data_dropped)  # Apply PCA to the scaled data

import numpy as np
print(np.isnan(pca_data).sum())  # Total number of NaN values

# Explained variance ratio for each component
explained_variance_ratio = pca.explained_variance_ratio_
print("Explained Variance Ratio:", explained_variance_ratio)

# Cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance_ratio)
print("Cumulative Explained Variance:", cumulative_explained_variance)

# Find the number of components needed for 90% variance
n_components = np.argmax(cumulative_explained_variance >= 0.9) + 1
print(f"Number of components needed to retain 90% variance: {n_components}")

explained_variance_ratio = pca.explained_variance_ratio_
print("Explained Variance Ratio for 9 Components:", explained_variance_ratio)

cumulative_variance = np.cumsum(explained_variance_ratio)
print("Cumulative Explained Variance:", cumulative_variance)

"""**Step 6: Model Selection**

*K-Means Clustering*
"""

# K-Means Clustering
kmeans = KMeans(n_clusters=4, random_state=42)  # Calling KMeans with 4 clusters
kmeans_labels = kmeans.fit_predict(pca_data)  # Apply KMeans clustering to the scaled data

kmeans.n_clusters

# Silhouette Score for K-Means
silhouette_kmeans = silhouette_score(pca_data, kmeans_labels)  # Compute silhouette score for K-Means
print(f"K-Means Silhouette Score: {silhouette_kmeans}")  # Display silhouette score for K-Means

# After fitting KMeans, access the centroids (which are in the PCA space)
reduced_centroids = kmeans.cluster_centers_

# Check the shape of the centroids
print(reduced_centroids.shape)  # Should output (4, 9) since there are 4 clusters and 9 components

print(f"Number of clusters: {kmeans.n_clusters}")

print(f"Number of PCA components: {pca.n_components_}")

# Example of labeling based on the centroid analysis (you would adjust this based on the actual values you observe)
cluster_labels = {
    0: "High Spenders",
    1: "Low Frequency Shoppers",
    2: "Occasional Shoppers",
    3: "Frequent Bargain Shoppers"
}

# Map the cluster labels to the clusters in the data
labels = [cluster_labels[label] for label in kmeans_labels]

# Add the cluster labels to the original data (optional)
customer_data_with_labels = pd.DataFrame(pca_data, columns=[f"PC{i+1}" for i in range(pca_data.shape[1])])
customer_data_with_labels['Cluster'] = labels
print(customer_data_with_labels.head())

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))

# Scatter plot of the data points colored by their clusters
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)
plt.title("KMeans Clustering of Customers (2D PCA projection)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

# Assuming the scaler was trained with 16 features, fit it again with the updated 17 features data
scaler = StandardScaler()
scaler.fit(scaled_data_dropped)  # 'updated_data' should have 17 features

import pandas as pd

# Assuming 'df' is your preprocessed dataframe
customer.to_csv("scaled_data_droped.csv", index=False)
from google.colab import files
files.download("scaled_data_droped.csv")  # Download locally

import joblib

# Save the trained KMeans model
joblib.dump(kmeans, "kmeans_model.pkl")
files.download("kmeans_model.pkl")  # Download locally

# Save the trained KMeans model
joblib.dump(pca, "pca_model.pkl")
files.download("pca_model.pkl")  # Download locally

# Save the scaler
joblib.dump(scaler, "scaler.pkl")
files.download("scaler.pkl")  # Download locally

# Save the standardscaler
joblib.dump(standardscaler, "standardscaler.pkl")
files.download("standardscaler.pkl")  # Download locally

"""***Creating new dataset for dashboard by adding new column of clusters ***"""

import pandas as pd
import joblib
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load your existing data
customer_data = pd.read_csv("Customer Data.csv")  # Replace with your actual file name

# Drop rows with NaN values (or handle them as per your requirements)
customer_data_cleaned = customer_data.dropna()

# Separate features (excluding CUST_ID and CASH_ADVANCE_FREQUENCY if needed)
features = customer_data_cleaned.drop(columns=["CUST_ID", "CASH_ADVANCE_FREQUENCY"])

# Scale the data (StandardScaler should be fitted to the training data)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(features)

# Train KMeans model with all 16 features
kmeans_model = KMeans(n_clusters=4, random_state=42)  # You can adjust the number of clusters
kmeans_model.fit(scaled_data)

# Save the updated KMeans model and scaler
joblib.dump(kmeans_model, "kmeans_model_updated.pkl")
joblib.dump(scaler, "scaler_updated.pkl")

# Now you can use the updated model and scaler to make predictions on new data
print("Model and scaler have been retrained and saved.")

# Load the updated KMeans model and scaler
kmeans_model_updated = joblib.load("kmeans_model_updated.pkl")
scaler_updated = joblib.load("scaler_updated.pkl")

# Load your existing data
customer_data = pd.read_csv("Customer Data.csv")  # Replace with your actual file name

# Drop rows with NaN values (or handle them as per your requirements)
customer_data_cleaned = customer_data.dropna()

# Separate features (excluding CUST_ID and CASH_ADVANCE_FREQUENCY if needed)
features = customer_data_cleaned.drop(columns=["CUST_ID", "CASH_ADVANCE_FREQUENCY"])

# Scale the data using the updated scaler
scaled_data = scaler_updated.transform(features)

# Predict clusters using the updated KMeans model
predicted_labels = kmeans_model_updated.predict(scaled_data)

# Define the cluster labels (update as per your analysis)
cluster_labels = {
    0: "High Spenders",
    1: "Low Frequency Shoppers",
    2: "Occasional Shoppers",
    3: "Frequent Bargain Shoppers"
}

# Map the predicted cluster labels to the cluster types
customer_data_cleaned['Cluster_Type'] = [cluster_labels[label] for label in predicted_labels]

# Save the updated data to a new CSV file
customer_data_cleaned.to_csv("customer_data_with_cluster_type_updated.csv", index=False)